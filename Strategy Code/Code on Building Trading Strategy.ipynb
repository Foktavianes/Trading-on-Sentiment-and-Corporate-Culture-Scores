{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6d5d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\fokta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import warnings\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8c9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 3)\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "mpl.rcParams[\"axes.grid\"] = True\n",
    "mpl.rcParams[\"grid.color\"] = \"grey\"\n",
    "mpl.rcParams[\"grid.alpha\"] = 0.25\n",
    "\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "\n",
    "mpl.rcParams[\"legend.fontsize\"] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_companies = pd.read_csv(\"list_of_companies.csv\")\n",
    "list_of_companies = list_of_companies.drop([\"Unnamed: 0\"], axis = 1)\n",
    "list_of_companies[\"month\"] = pd.to_datetime(list_of_companies[\"dates\"]).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb251b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_500 = pd.read_csv(\"SPX_500.csv\")\n",
    "spx_500_companies = spx_500.drop([\"Unnamed: 0\"], axis = 1)\n",
    "spx_500_companies[\"month\"] = pd.to_datetime(spx_500_companies[\"dates\"]).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f8b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_scores_tf = pd.read_csv(\"firm_scores_TF.csv\")\n",
    "companies_scores_tf = companies_scores_tf.drop([\"time\"], axis = 1)\n",
    "companies_scores_tf[\"year\"] = list_of_companies[\"year\"]\n",
    "companies_scores_tf[\"month\"] = list_of_companies[\"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3742e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_scores_tf.insert(0, \"tickers\", list_of_companies[\"tickers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9e5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_companies_scores = companies_scores_tf[companies_scores_tf[\"firm_id\"].isin(spx_500_companies[\"company_names\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c0d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_yearly_scores = (spx_companies_scores.groupby([\"tickers\", \"year\"]).mean()).drop([\"document_length\", \"month\"], axis = 1).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccb99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dataframe = pd.DataFrame()\n",
    "\n",
    "def yearly_data(data):\n",
    "    for i in data.columns:\n",
    "        ticker_dataframe[\"top_{}\".format(i)] = data.sort_values(by = i, ascending = False)[i].dropna().iloc[ :30].index.to_list()\n",
    "        ticker_dataframe[\"bot_{}\".format(i)] = data.sort_values(by = i, ascending = False)[i].dropna().iloc[-30: ].index.to_list()\n",
    "    return ticker_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71082c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tickers = yearly_data(spx_yearly_scores.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93b801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_sentiment = pd.read_csv(\"pca_pls.csv\")\n",
    "market_sentiment[\"date\"] = pd.to_datetime(market_sentiment[\"Unnamed: 0\"], format = \"%Y%m\")\n",
    "market_sentiment = market_sentiment.drop([\"Unnamed: 0\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3221c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_news = pd.read_csv(r\"Pos_News.csv\", sep = \"|\")\n",
    "positive_news.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07fbe390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokta\\AppData\\Local\\Temp\\ipykernel_23516\\2606595773.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spx_sentiment[\"year\"] = pd.to_datetime(spx_sentiment['date']).dt.year\n",
      "C:\\Users\\fokta\\AppData\\Local\\Temp\\ipykernel_23516\\2606595773.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spx_sentiment[\"month\"] = pd.to_datetime(spx_sentiment['date']).dt.month\n"
     ]
    }
   ],
   "source": [
    "spx_sentiment = positive_news[positive_news[\"ticker\"].isin(spx_500_companies[\"tickers\"])]\n",
    "spx_sentiment[\"year\"] = pd.to_datetime(spx_sentiment['date']).dt.year\n",
    "spx_sentiment[\"month\"] = pd.to_datetime(spx_sentiment['date']).dt.month\n",
    "spx_sentiment_higher_than_70 = spx_sentiment[spx_sentiment[\"sent_lex\"] > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5491f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokta\\AppData\\Local\\Temp\\ipykernel_23516\\3484070707.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  spx_sentiment_higher_than_70[\"dummy_date\"] = pd.to_datetime(new_date, format = \"%Y-%m\")\n"
     ]
    }
   ],
   "source": [
    "new_date = []\n",
    "\n",
    "for i in spx_sentiment_higher_than_70[\"date\"]:\n",
    "    new_date.append(i[ :-3])\n",
    "\n",
    "spx_sentiment_higher_than_70[\"dummy_date\"] = pd.to_datetime(new_date, format = \"%Y-%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edcb0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(spx_sentiment_higher_than_70, \n",
    "                  market_sentiment,\n",
    "                 left_on = 'dummy_date',   \n",
    "                 right_on = 'date')\n",
    "merged_data = merged_data.drop([\"date_y\",\"dummy_date\"], axis = 1)\n",
    "merged_data.rename(columns = {\"date_x\" : \"date\"}, inplace = True)\n",
    "merged_data = merged_data[merged_data[\"pls\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60299069",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "\n",
    "for tickers in list_of_tickers.columns[ :-2]:\n",
    "    ticker_filtering = merged_data[merged_data[\"ticker\"].isin(list_of_tickers[tickers])]\n",
    "    year_filter = ticker_filtering[ticker_filtering[\"year\"] < (int(tickers[-4:]) + 2)]\n",
    "    year_filter_2 = year_filter[year_filter[\"year\"] > int(tickers[-4:])]\n",
    "    dictionary[tickers] = pd.DataFrame(year_filter_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876b7eb",
   "metadata": {},
   "source": [
    "# Determining holding period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4d9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_change_in_event_return = pd.DataFrame(spx_sentiment_higher_than_70.iloc[ : , 6:9].T.diff().mean(axis =1))*100\n",
    "pct_change_in_event_return.columns = [\"pct_change_in_event_return\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580b480",
   "metadata": {},
   "source": [
    "# Top 30 firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0990920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_data(tickers, start, end): \n",
    "    df = yf.download(tickers.to_list(),\n",
    "                  start = start,\n",
    "                   end = end)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144f408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n",
      "[*********************100%***********************]  30 of 30 completed\n"
     ]
    }
   ],
   "source": [
    "dictionary_2 = {}\n",
    "\n",
    "for tickers in list_of_tickers.columns[ :-2]:\n",
    "    data = getting_data(list_of_tickers[tickers],dt.datetime(2015,1,1),dt.datetime(2019,12,29))[\"Close\"]\n",
    "    dictionary_2[tickers] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac9ecacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary_3 = {}\n",
    "\n",
    "for name, data in dictionary_2.items():\n",
    "    dictionary_2[name].index = dictionary_2[name].index.strftime(\"%Y-%m-%d\")\n",
    "    dataframe = dictionary_2[name].T[dictionary_2[name].T.index.isin(dictionary_2[name].T.index[dictionary_2[name].T.index.isin(dictionary[name][\"ticker\"])])]\n",
    "    dictionary_3[name] = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dd74077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_return(dataframe, shift_value, main_dataframe,groupby_dataframe):\n",
    "    returns = np.log(dataframe / dataframe.shift(1))\n",
    "    returns_next_day = returns.shift(shift_value)\n",
    "    first_filtering = returns_next_day[returns_next_day.index.isin(main_dataframe)]\n",
    "    second_filtering = first_filtering.unstack()[first_filtering.unstack().index.isin(groupby_dataframe)]\n",
    "    making_list = (second_filtering.sort_index().values.tolist())\n",
    "    return making_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1037fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_value =  [-1,-4]\n",
    "empty_list = []\n",
    "\n",
    "for i in shift_value:\n",
    "    for name, data in dictionary_2.items():\n",
    "        returns = event_return(dictionary_3[name].T, i, dictionary[name][\"date\"], dictionary[name].groupby([\"ticker\",\"date\"]).sum().index)\n",
    "        empty_list.append(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2896b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_30_returns = pd.concat([pd.Series(empty_list[0]) * 1 + pd.Series(empty_list[8]) * -1,\\\n",
    "                            pd.Series(empty_list[2]) * 1 + pd.Series(empty_list[10]) * -1,\\\n",
    "                            pd.Series(empty_list[4]) * 1 + pd.Series(empty_list[12]) * -1,\\\n",
    "                            pd.Series(empty_list[6]) * 1 + pd.Series(empty_list[14]) * -1], axis = 1)\n",
    "top_30_returns.columns = [\"2016\",\"2017\",\"2018\",\"2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5989882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe(data):\n",
    "    sharpe_ratio = (data.mean()/data.std()) *252**0.5\n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea55a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns = []\n",
    "sharpe_ratios = []\n",
    "\n",
    "for i in top_30_returns.columns:\n",
    "    cumulative_return = top_30_returns.cumsum().apply(np.exp)[i].dropna().iloc[-1] - 1\n",
    "    sharpe_ratio = sharpe(top_30_returns[i])\n",
    "    cumulative_returns.append(cumulative_return)\n",
    "    sharpe_ratios.append(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c766c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_30_returns = pd.concat([pd.Series(empty_list[1]) * 1 + pd.Series(empty_list[9]) * -1,\\\n",
    "                            pd.Series(empty_list[3]) * 1 + pd.Series(empty_list[11]) * -1,\\\n",
    "                            pd.Series(empty_list[5]) * 1 + pd.Series(empty_list[13]) * -1,\\\n",
    "                            pd.Series(empty_list[7]) * 1 + pd.Series(empty_list[15]) * -1], axis = 1)\n",
    "bottom_30_returns.columns = [\"2016\",\"2017\",\"2018\",\"2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4d7653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns_bottom = []\n",
    "sharpe_ratios_bottom = []\n",
    "\n",
    "for i in bottom_30_returns.columns:\n",
    "    cumulative_return = bottom_30_returns.cumsum().apply(np.exp)[i].dropna().iloc[-1] - 1\n",
    "    sharpe_ratio = sharpe(bottom_30_returns[i])\n",
    "    cumulative_returns_bottom.append(cumulative_return)\n",
    "    sharpe_ratios_bottom.append(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3703a26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top_30_cumulative_return</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_30_sharpe_ratio</th>\n",
       "      <td>-0.033</td>\n",
       "      <td>1.479</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottom_30_cumulative_return</th>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottom_30_sharpe_ratio</th>\n",
       "      <td>-2.493</td>\n",
       "      <td>-2.484</td>\n",
       "      <td>-8.221</td>\n",
       "      <td>-0.130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              2016   2017   2018   2019\n",
       "top_30_cumulative_return    -0.003  0.198 -0.018  0.127\n",
       "top_30_sharpe_ratio         -0.033  1.479 -0.117  0.682\n",
       "bottom_30_cumulative_return -0.128 -0.121 -0.228 -0.011\n",
       "bottom_30_sharpe_ratio      -2.493 -2.484 -8.221 -0.130"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table = pd.concat([pd.Series(cumulative_returns), pd.Series(sharpe_ratios), pd.Series(cumulative_returns_bottom), pd.Series(sharpe_ratios_bottom)], axis =1).T\n",
    "final_table.index = [\"top_30_cumulative_return\", \"top_30_sharpe_ratio\", \"bottom_30_cumulative_return\", \"bottom_30_sharpe_ratio\" ]\n",
    "final_table.columns = [\"2016\", \"2017\", \"2018\", \"2019\"]\n",
    "final_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
